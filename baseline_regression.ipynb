{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/whiteRa2bit/anaconda3/lib/python3.7/site-packages (0.90)\r\n",
      "Requirement already satisfied: numpy in /home/whiteRa2bit/anaconda3/lib/python3.7/site-packages (from xgboost) (1.17.4)\r\n",
      "Requirement already satisfied: scipy in /home/whiteRa2bit/anaconda3/lib/python3.7/site-packages (from xgboost) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess\n",
    "\n",
    "X_train, X_test, y_train = preprocess(df_train, df_test, use_ohe=True, use_scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipe = Pipeline([\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "lasso_pipe = Pipeline([\n",
    "    ('lasso', Lasso())\n",
    "])\n",
    "\n",
    "hb_pipe = Pipeline([\n",
    "    ('hb', HuberRegressor())\n",
    "])\n",
    "\n",
    "xgb_pipe = Pipeline([\n",
    "    ('xgb', xgb.XGBRegressor())\n",
    "])\n",
    "\n",
    "dt_pipe = Pipeline([\n",
    "    ('dt', DecisionTreeRegressor())\n",
    "])\n",
    "\n",
    "# svr_pipe = Pipeline([\n",
    "#     ('svr', SVR())\n",
    "# ])\n",
    "\n",
    "pa_pipe = Pipeline([\n",
    "    ('pa', PassiveAggressiveRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [ridge_pipe, lasso_pipe, hb_pipe, xgb_pipe, dt_pipe, pa_pipe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(X, y, pipeline):\n",
    "    rskf = StratifiedKFold(n_splits=5, random_state=1)\n",
    "    rmse_scores = []\n",
    "    for train_index, test_index in rskf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_test, np.round(pipeline.predict(X_test)))))\n",
    "\n",
    "    print(\"kfolds rmse: {0}, mean rmse: {1}\".format(\n",
    "        str([str(round(x, 3)) for x in sorted(rmse_scores)]),\n",
    "        round(np.mean(rmse_scores), 3)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Ridge\n",
      "kfolds rmse: ['1.162', '1.163', '1.167', '1.169', '1.17'], mean rmse: 1.166\n",
      "-----------------------------------------------------------------------\n",
      "Model: Lasso\n",
      "kfolds rmse: ['1.392', '1.392', '1.392', '1.392', '1.392'], mean rmse: 1.392\n",
      "-----------------------------------------------------------------------\n",
      "Model: HuberRegressor\n",
      "kfolds rmse: ['1.17', '1.172', '1.173', '1.174', '1.176'], mean rmse: 1.173\n",
      "-----------------------------------------------------------------------\n",
      "Model: XGBRegressor\n",
      "[16:04:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiteRa2bit/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/whiteRa2bit/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:04:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiteRa2bit/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:04:52] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiteRa2bit/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:05:04] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiteRa2bit/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:05:17] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "kfolds rmse: ['1.033', '1.034', '1.035', '1.041', '1.043'], mean rmse: 1.037\n",
      "-----------------------------------------------------------------------\n",
      "Model: DecisionTreeRegressor\n",
      "kfolds rmse: ['1.265', '1.268', '1.268', '1.269', '1.27'], mean rmse: 1.268\n",
      "-----------------------------------------------------------------------\n",
      "Model: PassiveAggressiveRegressor\n",
      "kfolds rmse: ['1.549', '1.709', '1.828', '1.894', '1.938'], mean rmse: 1.784\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "objects_to_calculate = 100000\n",
    "for pipeline in pipelines:\n",
    "    print(\"Model:\", type(pipeline[0]).__name__)\n",
    "    test_pipeline(X_train[:objects_to_calculate], y_train[:objects_to_calculate], pipeline)\n",
    "    print('-----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (\"bnb\", bnb_pipe),\n",
    "    (\"dt\", dt_pipe),\n",
    "    (\"etc\", etc_pipe),\n",
    "    #(\"gnb\", gnb_pipe),\n",
    "    (\"kn\", kn_pipe),\n",
    "    #(\"svc\", svc_pipe),\n",
    "    #(\"lr\", lr_pipe),\n",
    "    (\"rf\", rf_pipe),\n",
    "]\n",
    "\n",
    "mixed_pipe = Pipeline([\n",
    "    (\"voting\", VotingClassifier(classifiers, voting=\"soft\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfolds rmse: ['1.208', '1.214', '1.215', '1.227', '1.228'], mean rmse: 1.218\n"
     ]
    }
   ],
   "source": [
    "test_pipeline(X_train_transformed.values[:objects_to_calculate], y_train[:objects_to_calculate], mixed_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from itertools import cycle\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class ShuffleVoter(BaseEstimator, ClassifierMixin):  \n",
    "    \"\"\"\n",
    "       scikit-learn based voting aggregation ensembling.\n",
    "       Using bootstrapping creates a set of models, differing only by which data sample they are fed\n",
    "       \"\"\"\n",
    "\n",
    "    def __init__(self, models):\n",
    "        \"\"\"\n",
    "        model - base model ( or a pipeline ) ( unfitted )\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        \n",
    "    def ensemble_predictions(self, predictions, weights, type_=\"harmonic\"):\n",
    "        \"\"\"\n",
    "        Combines probabilistic class estimates using a variety of strategies.\n",
    "        Linear, harmonic, geometric and rank averaging are supported at this moment. \n",
    "        Insipred by well known Abhishek's kernel on Kaggle \n",
    "        model - base model ( or a pipeline ) ( unfitted )\n",
    "        \"\"\"\n",
    "        assert np.isclose(np.sum(weights), 1.0)\n",
    "        if type_ == \"linear\":\n",
    "            res = np.average(predictions, weights=weights, axis=0)\n",
    "        elif type_ == \"harmonic\":\n",
    "            res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n",
    "            return 1 / res\n",
    "        elif type_ == \"geometric\":\n",
    "            numerator = np.average(\n",
    "                [np.log(p) for p in predictions], weights=weights, axis=0\n",
    "            )\n",
    "            res = np.exp(numerator / sum(weights))\n",
    "            return res\n",
    "        elif type_ == \"rank\":\n",
    "            res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n",
    "            return res / (len(res) + 1)\n",
    "        return res\n",
    "\n",
    "\n",
    "    def fit( self, X, y, n_boots = 14, test_size = 100 ):\n",
    "        \"\"\"\n",
    "        n_boots - number of bootstrapping iterations ( and respective models built)\n",
    "        \"\"\"\n",
    "        self.clfs  = []\n",
    "        for i, model in zip(range(n_boots), cycle(self.models)):\n",
    "            X_tr, X_te, y_tr, y_te = train_test_split( X, y, test_size=test_size, random_state=3521 + i*10)\n",
    "\n",
    "            pa_clf = model\n",
    "            pa_clf.fit(X_tr, y_tr)\n",
    "\n",
    "            self.clfs.append(pa_clf)\n",
    "\n",
    "    def predict( self, X, ensemble_type = 'rank'):\n",
    "        # TODO: nonuniform weights\n",
    "        \n",
    "        n_boots = len( self.clfs)\n",
    "        preds = [ clf.predict(X) for clf in self.clfs ]\n",
    "        return self.ensemble_predictions( preds, np.ones(n_boots)*(1/float(n_boots)), ensemble_type)\n",
    "    \n",
    "    def predict_proba( self, X, ensemble_type = 'rank' ):\n",
    "        n_boots = len( self.clfs)\n",
    "        preds = [ clf.predict_proba(X) for clf in self.clfs ]\n",
    "        return self.ensemble_predictions( preds, np.ones(n_boots)*(1/float(n_boots)), ensemble_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
